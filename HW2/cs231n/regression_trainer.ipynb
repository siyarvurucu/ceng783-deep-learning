{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class ClassifierTrainer(object):\n",
    "  \"\"\" The trainer class performs SGD with momentum on a cost function \"\"\"\n",
    "  def __init__(self):\n",
    "    self.step_cache = {} # for storing velocities in momentum update\n",
    "\n",
    "  def train(self, X, y, X_val, y_val, \n",
    "            model, loss_function, \n",
    "            reg=0.0,\n",
    "            learning_rate=1e-2, momentum=0, learning_rate_decay=0.95,\n",
    "            update='momentum', sample_batches=True,\n",
    "            num_epochs=30, batch_size=100, acc_frequency=None,\n",
    "            verbose=False):\n",
    "    \"\"\"\n",
    "    Optimize the parameters of a model to minimize a loss function. We use\n",
    "    training data X and y to compute the loss and gradients, and periodically\n",
    "    check the accuracy on the validation set.\n",
    "\n",
    "    Inputs:\n",
    "    - X: Array of training data; each X[i] is a training sample.\n",
    "    - y: Vector of training labels; y[i] gives the label for X[i].\n",
    "    - X_val: Array of validation data\n",
    "    - y_val: Vector of validation labels\n",
    "    - model: Dictionary that maps parameter names to parameter values. Each\n",
    "      parameter value is a numpy array.\n",
    "    - loss_function: A function that can be called in the following ways:\n",
    "      scores = loss_function(X, model, reg=reg)\n",
    "      loss, grads = loss_function(X, model, y, reg=reg)\n",
    "    - reg: Regularization strength. This will be passed to the loss function.\n",
    "    - learning_rate: Initial learning rate to use.\n",
    "    - momentum: Parameter to use for momentum updates.\n",
    "    - learning_rate_decay: The learning rate is multiplied by this after each\n",
    "      epoch.\n",
    "    - update: The update rule to use. One of 'sgd', 'momentum', or 'rmsprop'.\n",
    "    - sample_batches: If True, use a minibatch of data for each parameter update\n",
    "      (stochastic gradient descent); if False, use the entire training set for\n",
    "      each parameter update (gradient descent).\n",
    "    - num_epochs: The number of epochs to take over the training data.\n",
    "    - batch_size: The number of training samples to use at each iteration.\n",
    "    - acc_frequency: If set to an integer, we compute the training and\n",
    "      validation set error after every acc_frequency iterations.\n",
    "    - verbose: If True, print status after each epoch.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - best_model: The model that got the highest validation accuracy during\n",
    "      training.\n",
    "    - loss_history: List containing the value of the loss function at each\n",
    "      iteration.\n",
    "    - train_acc_history: List storing the training set accuracy at each epoch.\n",
    "    - val_acc_history: List storing the validation set accuracy at each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    N = X.shape[0]\n",
    "\n",
    "    if sample_batches:\n",
    "      iterations_per_epoch = N / batch_size # using SGD\n",
    "    else:\n",
    "      iterations_per_epoch = 1 # using GD\n",
    "    num_iters = num_epochs * iterations_per_epoch\n",
    "    epoch = 0\n",
    "    best_val_acc = 0.0\n",
    "    best_model = {}\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    for it in xrange(num_iters):\n",
    "      if it % 10 == 0:  print 'starting iteration ', it\n",
    "\n",
    "      # get batch of data\n",
    "      if sample_batches:\n",
    "        batch_mask = np.random.choice(N, batch_size)\n",
    "        X_batch = X[batch_mask]\n",
    "        y_batch = y[batch_mask]\n",
    "      else:\n",
    "        # no SGD used, full gradient descent\n",
    "        X_batch = X\n",
    "        y_batch = y\n",
    "\n",
    "      # evaluate cost and gradient\n",
    "      cost, grads = loss_function(X_batch, model, y_batch, reg)\n",
    "      loss_history.append(cost)\n",
    "\n",
    "      cache = 0\n",
    "\n",
    "      # perform a parameter update\n",
    "      for p in model:\n",
    "        # compute the parameter step\n",
    "        if update == 'sgd':\n",
    "          dx = -learning_rate * grads[p]\n",
    "\n",
    "        elif update == 'momentum':\n",
    "          if not p in self.step_cache: \n",
    "            self.step_cache[p] = np.zeros(grads[p].shape)\n",
    "\n",
    "          #####################################################################\n",
    "          # Momentum                                                          #\n",
    "          #####################################################################\n",
    "          self.step_cache[p] = momentum * self.step_cache[p] - learning_rate * grads[p]\n",
    "          dx = self.step_cache[p]\n",
    "\n",
    "        elif update == 'rmsprop':\n",
    "          decay_rate = 0.99 # you could also make this an option TODO\n",
    "          if not p in self.step_cache: \n",
    "            self.step_cache[p] = np.zeros(grads[p].shape)\n",
    "          dx = np.zeros_like(grads[p]) # you can remove this after\n",
    "          #####################################################################\n",
    "          # RMSProp                                                           #\n",
    "          #####################################################################\n",
    "          self.step_cache[p] = decay_rate * self.step_cache[p] + (1 - decay_rate) * grads[p]**2\n",
    "          dx = - learning_rate * grads[p] / np.sqrt(self.step_cache[p] + 1e-8)\n",
    "\n",
    "        else:\n",
    "          raise ValueError('Unrecognized update type \"%s\"' % update)\n",
    "\n",
    "        # update the parameters\n",
    "        model[p] += dx\n",
    "\n",
    "      # every epoch perform an evaluation on the validation set\n",
    "      first_it = (it == 0)\n",
    "      epoch_end = (it + 1) % iterations_per_epoch == 0\n",
    "      acc_check = (acc_frequency is not None and it % acc_frequency == 0)\n",
    "      if first_it or epoch_end or acc_check:\n",
    "        if it > 0 and epoch_end:\n",
    "          # decay the learning rate\n",
    "          learning_rate *= learning_rate_decay\n",
    "          epoch += 1\n",
    "\n",
    "        # evaluate train accuracy\n",
    "        if N > 1000:\n",
    "          train_mask = np.random.choice(N, 1000)\n",
    "          X_train_subset = X[train_mask]\n",
    "          y_train_subset = y[train_mask]\n",
    "        else:\n",
    "          X_train_subset = X\n",
    "          y_train_subset = y\n",
    "        scores_train = loss_function(X_train_subset, model)\n",
    "        y_pred_train = np.argmax(scores_train, axis=1)\n",
    "        train_acc = np.mean(y_pred_train == y_train_subset)\n",
    "        train_acc_history.append(train_acc)\n",
    "\n",
    "        # evaluate val accuracy\n",
    "        scores_val = loss_function(X_val, model)\n",
    "        y_pred_val = np.argmax(scores_val, axis=1)\n",
    "        val_acc = np.mean(y_pred_val ==  y_val)\n",
    "        val_acc_history.append(val_acc)\n",
    "        \n",
    "        # keep track of the best model based on validation accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "          # make a copy of the model\n",
    "          best_val_acc = val_acc\n",
    "          best_model = {}\n",
    "          for p in model:\n",
    "            best_model[p] = model[p].copy()\n",
    "\n",
    "        # print progress if needed\n",
    "        if verbose:\n",
    "          print ('Finished epoch %d / %d: cost %f, train: %f, val %f, lr %e'\n",
    "                 % (epoch, num_epochs, cost, train_acc, val_acc, learning_rate))\n",
    "\n",
    "    if verbose:\n",
    "      print 'finished optimization. best validation accuracy: %f' % (best_val_acc, )\n",
    "    # return the best model and the training history statistics\n",
    "    return best_model, loss_history, train_acc_history, val_acc_history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
